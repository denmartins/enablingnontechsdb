{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Examples Outside the Database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import operator\n",
    "\n",
    "from minisom import MiniSom\n",
    "from scipy.spatial import distance as spd\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_neg(query, data, som, num_selected_items):\n",
    "    \"\"\"Select indexes based on their proximity in the map\"\"\"    \n",
    "    item_relevance_mapping = {}\n",
    "    winner = som.winner(query)\n",
    "    for index in range(len(data)):\n",
    "        elem = data[index]\n",
    "        w = som.winner(elem)\n",
    "        distance = spd.cityblock(list(winner), list(w))\n",
    "        item_relevance_mapping[index+1] = distance\n",
    "    \n",
    "    sorted_candidates = sorted(item_relevance_mapping.items(), key=operator.itemgetter(1))\n",
    "    positives = [x[0] for x in sorted_candidates[:num_selected_items]]\n",
    "\n",
    "    negatives = []\n",
    "    for j in range(1,num_selected_items+1):\n",
    "        negatives.append(sorted_candidates[-j][0])\n",
    "\n",
    "    return positives, negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(name_data, original_data, preprocessed_data, queries, nexperiments):\n",
    "    results = []\n",
    "    for query_id in range(len(queries)):\n",
    "        data = preprocessed_data.values\n",
    "        concept = original_data.query(queries[query_id]).index.to_list()\n",
    "\n",
    "        num_neurons = 5*(data.shape[0]**0.543)\n",
    "        x_size = int(num_neurons**0.5) +1\n",
    "        y_size = int(num_neurons**0.5) +1\n",
    "\n",
    "        learning_rate = 0.8\n",
    "        sigma = max(x_size, y_size)*0.5\n",
    "\n",
    "        som = MiniSom(x_size, y_size, data.shape[1], \n",
    "                    sigma=sigma, learning_rate=learning_rate, \n",
    "                    neighborhood_function='gaussian')\n",
    "\n",
    "        training_iterations = 1000\n",
    "\n",
    "        for fac in [1.0, 1.5, 2.0]:\n",
    "            for it in range(nexperiments):\n",
    "                e = random.choice(concept)\n",
    "                used_ids = [i for i in preprocessed_data.index if i != e]\n",
    "\n",
    "                labels = [int(x in concept) for x in range(1, preprocessed_data.shape[0]+1)]\n",
    "\n",
    "                query = data[int(e)-1]\n",
    "\n",
    "                data = preprocessed_data.loc[used_ids].values\n",
    "                y_train = [int(x in concept) for x in used_ids]\n",
    "\n",
    "                som.train_random(data, training_iterations, verbose=False)\n",
    "\n",
    "                pos, neg = get_pos_neg(query, data, som, int(len(concept)*fac))\n",
    "\n",
    "                predicted = [int(i in pos) for i in preprocessed_data.index]\n",
    "                y_test = labels\n",
    "\n",
    "                scores = precision_recall_fscore_support(y_test, predicted, average='binary')\n",
    "                report = list(scores[:3]) + [query_id, fac, 'SOM']\n",
    "\n",
    "                results.append(report)\n",
    "\n",
    "    df = pd.DataFrame(data=results, columns=['precision', 'recall', 'f1score', 'query_id', 'selfactor', 'estimator'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cartable = pd.read_pickle(os.path.join('datasets', 'car_original_dataset.pkl'))\n",
    "cartable.columns = [str.lower(col.replace('.', '_')) for col in cartable.columns]\n",
    "cartable['origin'] = cartable['origin'].map({0: False, 1: True})\n",
    "cartable['automatic_gearbox'] = cartable['automatic_gearbox'].map({0: False, 1: True})\n",
    "\n",
    "preprocessed_data = pd.read_pickle(os.path.join('datasets', '1993CarsPrep.pkl'))\n",
    "\n",
    "queries = [\n",
    "    \"type != 'Sporty' and origin == 1\",\n",
    "    \"automatic_gearbox == 1 and horsepower >= 150\",\n",
    "    \"price <= 7000 and mpg >= 26 and automatic_gearbox == 0\",\n",
    "    \"manufacturer == 'Ford' or manufacturer == 'Chevrolet'\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_exp = experiment('1993Cars', cartable, preprocessed_data, queries, nexperiments=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_exp.groupby(['query_id','selfactor','estimator']).mean()[['f1score', 'precision', 'recall']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
